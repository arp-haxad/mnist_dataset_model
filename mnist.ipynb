{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\arpit\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\nDl Completed...: 0 url [00:00, ? url/s]\nDl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\nDl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\nDl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\nDl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\nDl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\nDl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\nDl Size...:   0%|          | 0/1 [00:00<?, ? MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\nDl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n\nDl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.58 url/s]\nDl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n\nDl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.58 url/s]\nDl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n\nExtraction completed...:   0%|          | 0/1 [00:00<?, ? file/s]\u001b[A\u001b[A\n\nDl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.58 url/s]\nDl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n\nDl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.58 url/s]\nDl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n\nDl Completed...:  50%|█████     | 2/4 [00:00<00:00,  2.74 url/s]\nDl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n\nDl Completed...:  50%|█████     | 2/4 [00:00<00:00,  2.74 url/s]\nDl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n\nExtraction completed...:  50%|█████     | 1/2 [00:00<00:00,  1.90 file/s]\u001b[A\u001b[A\n\nDl Completed...:  50%|█████     | 2/4 [00:00<00:00,  2.74 url/s]\nDl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n\nExtraction completed...: 100%|██████████| 2/2 [00:00<00:00,  2.15 file/s]\u001b[A\u001b[A\nDl Completed...:  50%|█████     | 2/4 [00:00<00:00,  2.74 url/s]\nDl Size...:  10%|█         | 1/10 [00:00<00:08,  1.11 MiB/s]\u001b[A\n\nDl Completed...:  50%|█████     | 2/4 [00:00<00:00,  2.74 url/s]\nDl Size...:  20%|██        | 2/10 [00:00<00:07,  1.11 MiB/s]\u001b[A\n\nExtraction completed...: 100%|██████████| 2/2 [00:00<00:00,  2.15 file/s]\u001b[A\u001b[A\nDl Completed...:  50%|█████     | 2/4 [00:01<00:00,  2.74 url/s]\nDl Size...:  30%|███       | 3/10 [00:01<00:04,  1.49 MiB/s]\u001b[A\n\nDl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  2.44 url/s]\nDl Size...:  30%|███       | 3/10 [00:01<00:04,  1.49 MiB/s]\u001b[A\n\nDl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  2.44 url/s]\nDl Size...:  30%|███       | 3/10 [00:01<00:04,  1.49 MiB/s]\u001b[A\n\nExtraction completed...:  67%|██████▋   | 2/3 [00:01<00:00,  2.15 file/s]\u001b[A\u001b[A\nDl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  2.44 url/s]\nDl Size...:  40%|████      | 4/10 [00:01<00:03,  1.93 MiB/s]\u001b[A\n\nDl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  2.44 url/s]\nDl Size...:  50%|█████     | 5/10 [00:01<00:02,  1.93 MiB/s]\u001b[A\n\nExtraction completed...:  67%|██████▋   | 2/3 [00:01<00:00,  2.15 file/s]\u001b[A\u001b[A\nDl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  2.44 url/s]\nDl Size...:  60%|██████    | 6/10 [00:01<00:01,  2.56 MiB/s]\u001b[A\n\nExtraction completed...:  67%|██████▋   | 2/3 [00:01<00:00,  2.15 file/s]\u001b[A\u001b[A\n\nDl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  2.44 url/s]\nDl Size...:  60%|██████    | 6/10 [00:01<00:01,  2.56 MiB/s]\u001b[A\n\nExtraction completed...: 100%|██████████| 3/3 [00:01<00:00,  1.68 file/s]\u001b[A\u001b[A\nDl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  2.44 url/s]\nDl Size...:  70%|███████   | 7/10 [00:01<00:01,  2.85 MiB/s]\u001b[A\n\nDl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  2.44 url/s]\nDl Size...:  80%|████████  | 8/10 [00:01<00:00,  2.85 MiB/s]\u001b[A\n\nExtraction completed...: 100%|██████████| 3/3 [00:01<00:00,  1.68 file/s]\u001b[A\u001b[A\nDl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  2.44 url/s]\nDl Size...:  90%|█████████ | 9/10 [00:01<00:00,  3.76 MiB/s]\u001b[A\n\nExtraction completed...: 100%|██████████| 3/3 [00:01<00:00,  1.68 file/s]\u001b[A\u001b[A\nDl Completed...:  75%|███████▌  | 3/4 [00:02<00:00,  2.44 url/s]\nDl Size...: 100%|██████████| 10/10 [00:02<00:00,  4.50 MiB/s]\u001b[A\n\nDl Completed...: 100%|██████████| 4/4 [00:03<00:00,  1.20 url/s]\nDl Size...: 100%|██████████| 10/10 [00:03<00:00,  4.50 MiB/s]\u001b[A\n\nDl Completed...: 100%|██████████| 4/4 [00:03<00:00,  1.20 url/s]\nDl Size...: 100%|██████████| 10/10 [00:03<00:00,  4.50 MiB/s]\u001b[A\n\nExtraction completed...:  75%|███████▌  | 3/4 [00:03<00:00,  1.68 file/s]\u001b[A\u001b[A\n\nDl Completed...: 100%|██████████| 4/4 [00:04<00:00,  1.20 url/s]\nDl Size...: 100%|██████████| 10/10 [00:04<00:00,  4.50 MiB/s]\u001b[A\n\nExtraction completed...: 100%|██████████| 4/4 [00:04<00:00,  1.19s/ file]\nDl Size...: 100%|██████████| 10/10 [00:04<00:00,  2.10 MiB/s]\nDl Completed...: 100%|██████████| 4/4 [00:04<00:00,  1.19s/ url]\n0 examples [00:00, ? examples/s]\n\n\n  0%|          | 2/60000 [00:00<50:00, 19.99 examples/s]Shuffling and writing examples to C:\\Users\\arpit\\tensorflow_datasets\\mnist\\3.0.1.incompleteNCGSUZ\\mnist-train.tfrecord\n 91%|█████████▏| 9149/10000 [00:00<00:00, 42429.49 examples/s]Shuffling and writing examples to C:\\Users\\arpit\\tensorflow_datasets\\mnist\\3.0.1.incompleteNCGSUZ\\mnist-test.tfrecord\n\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\arpit\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
    }
   ],
   "source": [
    "mnist_data,mnist_info = tfds.load(name = 'mnist',with_info = True,as_supervised = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'test': <PrefetchDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>,\n 'train': <PrefetchDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>}"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tfds.core.DatasetInfo(\n    name='mnist',\n    version=3.0.1,\n    description='The MNIST database of handwritten digits.',\n    homepage='http://yann.lecun.com/exdb/mnist/',\n    features=FeaturesDict({\n        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n    }),\n    total_num_examples=70000,\n    splits={\n        'test': 10000,\n        'train': 60000,\n    },\n    supervised_keys=('image', 'label'),\n    citation=\"\"\"@article{lecun2010mnist,\n      title={MNIST handwritten digit database},\n      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n      volume={2},\n      year={2010}\n    }\"\"\",\n    redistribution_info=,\n)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "mnist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_data['train'],mnist_data['test']\n",
    "# define a validation set\n",
    "num_mnist_validation = 0.1*mnist_info.splits['train'].num_examples\n",
    "num_mnist_validation = tf.cast(mnist_validation,tf.int64)\n",
    "num_mnist_test_sample = mnist_info.splits['test'].num_examples\n",
    "num_mnist_test_sample = tf.cast(mnist_test_sample,tf.int64)\n",
    "# defining a function to scale the intensity of the images from 0 to 1\n",
    "def scale(image,label):\n",
    "    image   =  tf.cast(image,tf.float32)\n",
    "    image = image/255.\n",
    "    return image, label\n",
    "scaled_train_validation_data = mnist_train.map(scale) # scaling \n",
    "scaled_test_data = mnist_test.map(scale) # scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shuffling and batching the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "shuffled_validation_train_data = scaled_train_validation_data.shuffle(BUFFER_SIZE) # shuffle the whole data\n",
    "shuffled_validation_data = shuffled_validation_train_data.take(num_mnist_validation)\n",
    "shuffled_train_data = shuffled_validation_train_data.skip(num_mnist_validation)\n",
    "\n",
    "#batching\n",
    "BATCH_SIZE = 100\n",
    "shuffled_train_data = shuffled_train_data.batch(BATCH_SIZE)\n",
    "  #FOR Validation test data we need to specify the whole batch as one batch since we need to move forward and validation  do not have \n",
    "  #back propagation in or we dint use it in calculating  weight but still we should define a batch for validation data too\n",
    "shuffled_validation_data = shuffled_validation_data.batch(num_mnist_validation)\n",
    "test_data  = scaled_test_data.batch(num_mnist_test_sample)\n",
    "\n",
    "# extract validation inputs and targets\n",
    "validation_inputs, validation_targets = next(iter(shuffled_validation_data))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL\n",
    " #outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "model = tf.keras.Sequential([ \n",
    "            tf.keras.layers.Flatten(input_shape = (28,28,1)),\n",
    "            tf.keras.layers.Dense(hidden_layer_size,activation ='relu'),\n",
    "            tf.keras.layers.Dense(hidden_layer_size,activation ='relu'),\n",
    "            tf.keras.layers.Dense(output_size,activation='softmax')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choose the loss and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/5\n540/540 - 9s - loss: 0.4252 - accuracy: 0.8778 - val_loss: 0.2101 - val_accuracy: 0.9385\nEpoch 2/5\n540/540 - 3s - loss: 0.1902 - accuracy: 0.9448 - val_loss: 0.1510 - val_accuracy: 0.9558\nEpoch 3/5\n540/540 - 3s - loss: 0.1411 - accuracy: 0.9587 - val_loss: 0.1316 - val_accuracy: 0.9605\nEpoch 4/5\n540/540 - 3s - loss: 0.1153 - accuracy: 0.9657 - val_loss: 0.1221 - val_accuracy: 0.9667\nEpoch 5/5\n540/540 - 3s - loss: 0.0969 - accuracy: 0.9709 - val_loss: 0.0893 - val_accuracy: 0.9743\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x232232381c8>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "model.fit(shuffled_train_data,epochs = 5, validation_data=(validation_inputs,validation_targets),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1/1 [==============================] - 0s 4ms/step - loss: 0.1045 - accuracy: 0.9686\n"
    }
   ],
   "source": [
    "test_loss,test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "test loss 0.10. test accuracy96.86\n"
    }
   ],
   "source": [
    "print(' test loss {0:.2f}. test accuracy{1:.2f}'.format(test_loss,test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595821967607",
   "display_name": "Python 3.7.7 64-bit ('py3tf2.0': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}